{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "register_input_path = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "stations_input_path = \"/data/students/bigdata_internet/lab3/stations.csv\"\n",
    "register_w_header_rdd = sc.textFile(register_input_path)\n",
    "stations_w_header_rdd = sc.textFile(stations_input_path)\n",
    "outputPath = \"lab3RDD_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the header from the rdd created by reading the csv file\n",
    "register_w_erroneous_lines_rdd = register_w_header_rdd.filter(lambda line: line != \"station\\ttimestamp\\tused_slots\\tfree_slots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2008-05-15 12:01:00\t0\t18\n",
      "Number of reading before erroneous removal 25319028\n"
     ]
    }
   ],
   "source": [
    "# We can see the format of the input rdd now after removing the header line\n",
    "print(register_w_erroneous_lines_rdd.first())\n",
    "print('Number of reading before erroneous removal',register_w_erroneous_lines_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2008-05-15 12:01:00\t0\t18\n",
      "Number of reading after removal of erroneous ones 25104121\n"
     ]
    }
   ],
   "source": [
    "# need to remove erroneous lines with used == 0 and free == 0\n",
    "def checkIfErroneousLine(line):\n",
    "    station_id,timestamp,used,free = line.split(\"\\t\")\n",
    "    if int(used) == 0 and int(free) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "register_rdd = register_w_erroneous_lines_rdd.filter(checkIfErroneousLine)\n",
    "print(register_rdd.first())\n",
    "print('Number of reading after removal of erroneous ones',register_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes as input the register_rdd\n",
    "# 1 2008-05-15 12:01:00 0 18 (the values are seperated by tab)\n",
    "# splits the input line into station_id, timestamp (ymd,hms) , used_slots and free_slots\n",
    "# it then cheks if the free slots are zero which is the critical condition\n",
    "# if so it assigns a 1 value, else it assignes zero\n",
    "def createkv(line):\n",
    "    station_id,timestamp,used,free = line.split(\"\\t\")\n",
    "    # create a datetime object in order to first parse the input string (strptime string parse time)\n",
    "    dtobj = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "    # using the datetime object and formatting it with strftime (string format time)\n",
    "    weekday = dtobj.strftime('%A')\n",
    "    hour = dtobj.strftime(\"%H\")\n",
    "    timeslot = (weekday,hour)\n",
    "    critical = 0\n",
    "    if int(free) == 0:\n",
    "        critical = 1\n",
    "    else :\n",
    "        critical = 0\n",
    "    return ( (station_id,timeslot) , (critical,1) ) \n",
    "# in this way i can count the total occurences by giving as values the criticality 1/0 and the instances aka 1 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('99', ('Wednesday', '23')), (0, 1)), (('99', ('Wednesday', '23')), (0, 1))]\n"
     ]
    }
   ],
   "source": [
    "# rdd with station_id,time slot as keys and (1/0, 1) as the values that represent if critical or not and the 1 to count the occurances\n",
    "station_timeslot_values_rdd = register_rdd.map(createkv)\n",
    "print(station_timeslot_values_rdd.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total number of critical readings and total number of readings for a Si,Tj key pair\n",
    "def functt(v1,v2):\n",
    "    crit1 = v1[0]\n",
    "    count1 = v1[1]\n",
    "    crit2 = v2[0]\n",
    "    count2 = v2[1]\n",
    "    return (crit1+crit2,count1+count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('99', ('Wednesday', '23')), (0, 568)), (('99', ('Wednesday', '22')), (15, 568))]\n"
     ]
    }
   ],
   "source": [
    "# Using reduceByKey in order to calculate for each unique key pair of Si,Tj\n",
    "# the values necessary for the criticality, which are the total numer of critical readings\n",
    "# and the total readings for that pair\n",
    "criticality_instances_rdd = station_timeslot_values_rdd.reduceByKey(functt)\n",
    "print(criticality_instances_rdd.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('99', ('Wednesday', '23')), 0.0), (('99', ('Wednesday', '22')), 0.02640845070422535)]\n"
     ]
    }
   ],
   "source": [
    "# Using mapValues in order to apply a function on the values for each line of the input rdd\n",
    "# since we have reduced by key every line will have a unique key\n",
    "# the lambda function calculates the fraction of critical values over all readings (for a certain key Si,Tj)\n",
    "criticality_value_rdd = criticality_instances_rdd.mapValues(lambda critical_and_total: critical_and_total[0]/critical_and_total[1])\n",
    "print(criticality_value_rdd.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements that satisfy the filter condition 5\n",
      "[(('9', ('Friday', '22')), 0.6258389261744967), (('58', ('Monday', '00')), 0.6323119777158774)]\n"
     ]
    }
   ],
   "source": [
    "# Select only the pair that have criticality over a certain threshold defined by user\n",
    "THRESHOLD = 0.6\n",
    "# Function to pass to the filter transformation in order to \n",
    "# select only those lines that satisfy the threshold contraint on criticality\n",
    "def filterForThreshold(line,THRESHOLD):\n",
    "    # (('99', ('Wednesday', '23')), 0.0017574692442882249)\n",
    "    criticality_val = line[1]\n",
    "    if criticality_val>= THRESHOLD:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# Filtering \n",
    "filtered_criticality_rdd = criticality_value_rdd.filter(lambda line: filterForThreshold(line,THRESHOLD))\n",
    "print(\"Number of elements that satisfy the filter condition\",filtered_criticality_rdd.count())\n",
    "# Examples of the filtered RDD\n",
    "print(filtered_criticality_rdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.6323119777158774, 58, 'Monday', 0), 1),\n",
       " ((0.6258389261744967, 9, 'Friday', 22), 1),\n",
       " ((0.6239554317548747, 58, 'Monday', 1), 1),\n",
       " ((0.622107969151671, 10, 'Saturday', 0), 1),\n",
       " ((0.6129032258064516, 9, 'Friday', 10), 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapForSorting(line):\n",
    "    criticality = line[1]\n",
    "    return ((criticality,int(line[0][0]),line[0][1][0],int(line[0][1][1])),1)#criticality,sId,day,hour \n",
    "newKeysValue = filtered_criticality_rdd.map(mapForSorting)\n",
    "sortedRdd = newKeysValue.sortByKey(False)\n",
    "sortedRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(58, 'Monday', 0, 0.6323119777158774), (9, 'Friday', 22, 0.6258389261744967)]\n"
     ]
    }
   ],
   "source": [
    "def orderForDF(line):\n",
    "    return line[0][1],line[0][2],line[0][3],line[0][0] #station|weekday|hour|criticality|\n",
    "reordered_rdd = sortedRdd.map(orderForDF)\n",
    "print(reordered_rdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+------------------+\n",
      "|station|weekday|hour|       criticality|\n",
      "+-------+-------+----+------------------+\n",
      "|     58| Monday|   0|0.6323119777158774|\n",
      "+-------+-------+----+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(reordered_rdd,[\"station\",\"weekday\",\"hour\",\"criticality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+------------------+---------+---------+\n",
      "|station|weekday|hour|       criticality|longitude| latitude|\n",
      "+-------+-------+----+------------------+---------+---------+\n",
      "|     58| Monday|   0|0.6323119777158774| 2.170736|41.377536|\n",
      "|      9| Friday|  22|0.6258389261744967| 2.185294|41.385006|\n",
      "+-------+-------+----+------------------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df = spark.read.load(stations_input_path,\n",
    "                     format=\"csv\", header=True, inferSchema=True, sep=\"\\t\")\n",
    "sorted_criticality_w_coordinates_df = df.join(stations_df,df.station == stations_df.id, \"inner\")\n",
    "final_df = sorted_criticality_w_coordinates_df.select(\"station\",  \"weekday\",\"hour\", \"criticality\", \"longitude\" ,\"latitude\")\n",
    "final_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.csv(outputPath, header=True,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "####bonus#####\n",
    "stations_noheader = stations_w_header_rdd.filter(lambda line:line!='id\\tlongitude\\tlatitude\\tname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_usedOne=register_rdd.map(lambda line:(line.split(\"\\t\")[0], (int(line.split(\"\\t\")[2]),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumUsedOne(v1,v2):\n",
    "    u1 = v1[0]\n",
    "    i1 = v1[1]\n",
    "    u2 = v2[0]\n",
    "    i2 = v2[1]\n",
    "    return (u1+u2,i1+i2)\n",
    "station_totUsedTotInst = station_usedOne.reduceByKey(sumUsedOne)\n",
    "station_usedAvg = station_totUsedTotInst.mapValues(lambda val: val[0]/val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(line):\n",
    "    lat1 = 41.386904\n",
    "    lon1 = 2.169989\n",
    "    stationid,lon,lat,name = line.split(\"\\t\")\n",
    "    lon2=float(lon)\n",
    "    lat2=float(lat)\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 \n",
    "    d =  c * r\n",
    "    return (stationid,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_distToCentre = stations_noheader.map(haversine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lessthan1km = station_distToCentre.filter(lambda line: line[1]<1)\n",
    "Morethan1km = station_distToCentre.filter(lambda line: line[1]>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_usedAvg_distLessThanOne=station_usedAvg.join(Lessthan1km)\n",
    "station_usedAvg_distMoreThanOne=station_usedAvg.join(Morethan1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgUsed_distMoreThanOne = station_usedAvg_distMoreThanOne.map(lambda line: ('more',line[1][0]))\n",
    "avgUsed_distLessThanOne = station_usedAvg_distLessThanOne.map(lambda line: ('more',line[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avgUsed_distMoreThanOne = avgUsed_distMoreThanOne.reduceByKey(lambda acc,n:acc+n)\n",
    "total_avgUsed_distLessThanOne = avgUsed_distLessThanOne.reduceByKey(lambda acc,n:acc+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.905908792496716"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_avgUsed_distMoreThanOne.first()[1]/station_usedAvg_distMoreThanOne.count() # U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.223426232628029"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_avgUsed_distLessThanOne.first()[1]/station_usedAvg_distLessThanOne.count() # U2 further away stations are more used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
