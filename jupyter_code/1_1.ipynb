{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the textFile method to create an RDD from a text file\n",
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "# Question: Where is the input file? On which file system?\n",
    "# The data is stored in the hdfs (as seen from hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the map transformation is used to create a new RDD by applying\n",
    "# a function f on each element of the input RDD\n",
    "# for each element of the input RDD there is a corresponding element \n",
    "# in the output RDD\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a second map transformation takes the RDD that was given as\n",
    "# output from the previous map transformation \n",
    "# we take the second element of the line from the input RDD\n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce is an action so it will output a single python object\n",
    "# obtained by combining all the objects of the input RDD \n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    }
   ],
   "source": [
    "print(\"The sum is:\", value_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Which value is printed by the print statement? \n",
    "# The value given by the print statement is 46, which corresponds\n",
    "# to the sum of all the second elements from the tuples of the input RDD\n",
    "# Which is the purpose of each line of code?\n",
    "# See line by line the explenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Where is the input file? On which file system?\n",
    "# The input file is stored in the hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "\n",
    "# s274990@jupyter-s274990:~/newLabs/lab1$ pyspark --master local --deploy-mode client\n",
    "# WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
    "# WARNING: Running pyspark from user-defined location.\n",
    "# Python 3.7.5 (default, Oct 25 2019, 15:51:11) \n",
    "# [GCC 7.3.0] :: Anaconda, Inc. on linux\n",
    "# Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "# 20/08/04 16:23:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "# 20/08/04 16:23:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
    "# 20/08/04 16:23:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
    "# 20/08/04 16:23:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
    "# 20/08/04 16:23:26 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
    "# Welcome to\n",
    "#       ____              __\n",
    "#      / __/__  ___ _____/ /__\n",
    "#     _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "#    /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0-cdh6.2.1\n",
    "#       /_/\n",
    "\n",
    "# Using Python version 3.7.5 (default, Oct 25 2019 15:51:11)\n",
    "# SparkSession available as 'spark'.\n",
    "# >>> rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "# >>> fields_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "# >>> value_rdd = fields_rdd.map(lambda l: int(l[1]))\n",
    "# >>> value_sum = value_rdd.reduce(lambda v1, v2:v1+v2)\n",
    "# >>> print(\"the sum is:\", value_sum)\n",
    "# the sum is: 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The --master option specifies the master URL for a distributed cluster\n",
    "#Â  local to run locally with one thread\n",
    "# --deploy-mode client \n",
    "# Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"My app\")\n",
    "sc = SparkContext(conf=conf)\n",
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))\n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1 + v2)\n",
    "print(\"The sum is:\", value_sum)\n",
    "# Question: In which file system are located your script and the /data/students/bigdata_internet/lab1/lab1_dataset.txt files? \n",
    "# Are they on the same file system?\n",
    "# The script is in the local file system and the dataset is stored in the hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /data/students/bigdata_internet/lab1\n",
    "Found 1 items\n",
    "-rwxrwx---+  3 trevisan students         62 2019-09-06 12:15 /data/students/bigdata_internet/lab1/lab1_dataset.txt\n",
    "\n",
    "hdfs dfs -usage \n",
    "...\n",
    "...\n",
    "\n",
    "Now copy the HDFS file /data/students/bigdata_internet/lab1/lab1_dataset.txt in the local file system.\n",
    "Question: if you modify the local file, does the modifications automatically affect also the HDFS file?\n",
    "No it does not modify the file in the hdfs, but only that which is stored in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
