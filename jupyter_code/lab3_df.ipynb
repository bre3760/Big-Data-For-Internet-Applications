{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "register_input_path = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "stations_input_path = \"/data/students/bigdata_internet/lab3/stations.csv\"\n",
    "outputPath = \"lab3DF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from register.csv\n",
    "register_df = spark.read.load(register_input_path,\n",
    "                     format=\"csv\", header=True, inferSchema=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from stations.csv\n",
    "stations_df = spark.read.load(stations_input_path,\n",
    "                     format=\"csv\", header=True, inferSchema=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of lines in the register data frame\n",
    "register_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to remove the lines of the register_df that have errors\n",
    "# removing the erroneous lines where both used and free slots are zero \n",
    "filtered_register_df = register_df.filter(\"not(used_slots=0 and free_slots=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25104121"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_register_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeslot(timestamp):\n",
    "    # '2008-05-15 12:20:00' is the timestamp format and is a timestamp object of class datetime\n",
    "    weekday = (timestamp.strftime('%A'))\n",
    "    hour = (timestamp.strftime(\"%H\"))\n",
    "    timeslot = str(weekday) +' '+ str(hour)\n",
    "    return timeslot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"toWeekday\", lambda timestamp: (timestamp.strftime('%A')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"toHour\", lambda timestamp: timestamp.strftime('%H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"toTimeslot\", lambda timestamp: create_timeslot(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_w_weekday_hour_df = filtered_register_df.selectExpr(\"station\",\"toWeekday(timestamp) as weekday\",\"toHour(timestamp) as hour\", \"used_slots\",\"free_slots\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_df = filtered_register_df.selectExpr(\"station\",\"toTimeslot(timestamp) as timeslot\", \"used_slots\",\"free_slots\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkcriticality(free_slots):\n",
    "    critical = 0\n",
    "    if free_slots == 0: # do not have to cast to int since df infer schema already makes it integer\n",
    "        critical = 1\n",
    "    else :\n",
    "        critical = 0\n",
    "    return int(critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countreading(station):\n",
    "    return int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"CountReading\", lambda station: countreading(station) ,\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"isCritical\", lambda free_slots: checkcriticality(free_slots) ,\"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criticality_df = timestamp_df.selectExpr(\"station\",\"timeslot\",\"isCritical(free_slots) as critical\",\"CountReading(station) as reading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "statio_timeslot_criticaltotal_df = criticality_df.groupBy(\"station\",\"timeslot\").sum(\"critical\",\"reading\").withColumnRenamed(\"sum(critical)\",\"total_critical\")\\\n",
    "                                                                                                                            .withColumnRenamed(\"sum(reading)\",\"total_readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+--------------+\n",
      "|station|   timeslot|total_critical|total_readings|\n",
      "+-------+-----------+--------------+--------------+\n",
      "|    180|  Friday 18|             0|           597|\n",
      "|    180|Thursday 08|             3|           531|\n",
      "+-------+-----------+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statio_timeslot_criticaltotal_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCriticality(total_critical,total_readings):\n",
    "    criticality = float(total_critical/total_readings)\n",
    "    return criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"CalculateCriticality\", lambda total_critical,total_readings: calculateCriticality(total_critical,total_readings) ,\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_timeslot_criticality_df = statio_timeslot_criticaltotal_df.selectExpr(\"station\",\"timeslot\",\"CalculateCriticality(total_critical,total_readings) as criticality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_timeslot_criticality_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a threshold to filter the values with \n",
    "THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_criticality_df = station_timeslot_criticality_df.filter(\"criticality>{}\".format(THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|station|   timeslot|criticality|\n",
      "+-------+-----------+-----------+\n",
      "|      9|  Friday 10| 0.61290324|\n",
      "|     10|Saturday 00|   0.622108|\n",
      "+-------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_criticality_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_criticality_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_criticality_df = filtered_criticality_df.sort(\"criticality\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|station|   timeslot|criticality|\n",
      "+-------+-----------+-----------+\n",
      "|     58|  Monday 00|   0.632312|\n",
      "|      9|  Friday 22| 0.62583894|\n",
      "|     58|  Monday 01|  0.6239554|\n",
      "|     10|Saturday 00|   0.622108|\n",
      "|      9|  Friday 10| 0.61290324|\n",
      "+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_criticality_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ta.join(tb, ta.name == tb.name, 'inner'). \n",
    "sorted_criticality_w_coordinates_df = sorted_criticality_df.join(stations_df,sorted_criticality_df.station == stations_df.id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_criticality_w_coordinates_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sorted_criticality_w_coordinates_df.select(\"station\",  \"timeslot\", \"criticality\",  \"longitude\" ,\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+---------+---------+\n",
      "|station|   timeslot|criticality|longitude| latitude|\n",
      "+-------+-----------+-----------+---------+---------+\n",
      "|     58|  Monday 00|   0.632312| 2.170736|41.377536|\n",
      "|      9|  Friday 22| 0.62583894| 2.185294|41.385006|\n",
      "|     58|  Monday 01|  0.6239554| 2.170736|41.377536|\n",
      "|     10|Saturday 00|   0.622108| 2.185206|41.384875|\n",
      "|      9|  Friday 10| 0.61290324| 2.185294|41.385006|\n",
      "+-------+-----------+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.csv(outputPath, header=True,sep = \"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
